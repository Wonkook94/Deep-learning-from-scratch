{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78038bc",
   "metadata": {},
   "source": [
    "## 4.2.1 오차제곱합(Sum of Squares for Error, SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4829ae",
   "metadata": {},
   "source": [
    "$E={1\\over2}\\sum_k^n({y_k-t_k})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f4758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d187308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# y : 신경망의 출력(신경망이 추정한 값)\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# t : 정답 레이블 (one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d90da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995282e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 1\n",
    "# 신경망의 예측값 : 2 , 실제 값 : 2\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c6b254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6437499999999999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 2\n",
    "# 신경망의 예측값 : 7, 실제 값 : 2\n",
    "y = [0.1, 0.05, 0.05, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901e72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값이 실제 값과 다르면 오차의 합도 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e42f2",
   "metadata": {},
   "source": [
    "## 4.2.2 교차 엔트로피 오차 (Cross Entropy Error, CEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceec294",
   "metadata": {},
   "source": [
    "$E=-\\sum_k^nt_k\\log{y_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dee57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7    # log0 이 되는것을 방지해주는 역할\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9612458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 1\n",
    "# 신경망의 예측값 : 2 , 실제 값 : 2\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa32abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9957302735559908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 2\n",
    "# 신경망의 예측값 : 7, 실제 값 : 2\n",
    "y = [0.1, 0.05, 0.05, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30eca1",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508f4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "#sys.path.append('C:/Users/SWK/Desktop/Learning/Deep Learning/DeepLearning From Scratch/CH3') #절대 경로를 이용하여 경로 추가\n",
    "from dataset.mnist import load_mnist    #MNIST dataset load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c4e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef46b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)   # train_size에서 랜덤으로 batch_size 뽑는다. 뽑힌 수를 인덱스로 사용할 예정\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]\n",
    "\n",
    "print(x_batch)\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a938e7",
   "metadata": {},
   "source": [
    "## 4.2.4 배치용 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb88ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(t*np.log(y[np.arrange(batch_size),t] +1e-7)) / batch_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a439926",
   "metadata": {},
   "source": [
    "## 4.3.1 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6f517e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd9576",
   "metadata": {},
   "source": [
    "## 4.4.1 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf0d657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)   # x와 같은 형상의 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val-h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] =  (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f662977",
   "metadata": {},
   "source": [
    "## 경사하강법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45cf46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# f : 최적화 하려는 함수\n",
    "# init_x : 초기값\n",
    "# lr : learning rate \n",
    "# step_num : 반복횟수\n",
    "\n",
    "\n",
    "## Learning rate의 경우는 하이퍼파라미터이다.\n",
    "## 사람이 직접 설정을 해야한다.\n",
    "## 여러번의 실험을 통해 좋은 결과를 내는 값을 찾아야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e99b0",
   "metadata": {},
   "source": [
    "## 4.5.2신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63c4d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98bad357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16269039 -0.08194958 -2.08930297]\n",
      " [ 0.02094752  0.62479319  1.03270093]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48adc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11646701  0.51314412 -0.32415094]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b84634fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 1, 0])   # 정답 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfd09508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445199876690615"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6460cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19166176 -0.31502265  0.1233609 ]\n",
      " [ 0.28749263 -0.47253398  0.18504135]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dw = numerical_gradient(f, net.W)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e6691",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca01937",
   "metadata": {},
   "source": [
    "## 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a988942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}  # 신경망의 매개변수를 저장하는 딕셔너리 변수\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 1층의 가중치\n",
    "        self.params['b1'] = np.zeros(hidden_size) # 1층의 편향\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) # 2층의 가중치\n",
    "        self.params['b2'] = np.zeros(output_size) # 2층의 편향\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {} # 기울기 값을 저장하기 위한 딕셔러니 변수\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1']) # 1층 가중치에 대한 기울기\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1']) # 1층 편향에 대한 기울기\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2']) # 2층 가중치에 대한 기울기\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2']) # 2층 편향에 대한 기울기\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7a3c6",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2c36b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09863333333333334, 0.0958\n",
      "train acc, test acc | 0.7776333333333333, 0.7836\n",
      "train acc, test acc | 0.8784333333333333, 0.8804\n",
      "train acc, test acc | 0.89905, 0.9021\n",
      "train acc, test acc | 0.90785, 0.9115\n",
      "train acc, test acc | 0.9158166666666666, 0.9181\n",
      "train acc, test acc | 0.9199833333333334, 0.9246\n",
      "train acc, test acc | 0.9246166666666666, 0.9269\n",
      "train acc, test acc | 0.92745, 0.9292\n",
      "train acc, test acc | 0.9318166666666666, 0.9331\n",
      "train acc, test acc | 0.9350166666666667, 0.9346\n",
      "train acc, test acc | 0.9378, 0.9368\n",
      "train acc, test acc | 0.9401333333333334, 0.9408\n",
      "train acc, test acc | 0.9412833333333334, 0.9411\n",
      "train acc, test acc | 0.9431666666666667, 0.943\n",
      "train acc, test acc | 0.9459166666666666, 0.9448\n",
      "train acc, test acc | 0.9474166666666667, 0.9453\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
